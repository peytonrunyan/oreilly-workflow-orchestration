{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***demo***  - putting together a \"real world\" workflow with Prefect\n",
    "Let's look at how we can leverage the Prefect engine alongside a few of today's bleeding edge techologies to build a rudimentary end-to-end data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what does orchestrating a data pipeline look like?\n",
    "If we break it down at a high-level, we have a process that can:\n",
    "\n",
    "- trigger an **extract** of raw data sources from somewhere and drop them in our warehouse \n",
    "\n",
    "- **transform** that raw data somehow into clean, reporting-ready tables\n",
    "\n",
    "- provide visibility into any failures that occur in this process\n",
    "\n",
    "... and that process should ideally be easy to **manage**, **scale**, and **understand**! (this is where an orchestrator like Prefect comes in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è what's in our *stack*?\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/logos.png\" width=900/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <img src=\"imgs/marvin.png\" width=33/>  &emsp; [**prefect**](https://www.prefect.io) &emsp; our **workflow orchestrator**\n",
    "\n",
    "[Marvin](https://www.prefect.io/blog/introducing-marvins-challenge/), the resident blue rubber duck at Prefect, helps us turn vanilla python code into production-ready workflows.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/prefect-orca.svg\" width=500/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <img src=\"imgs/airbyte.png\" width=40/>  &ensp; [**airbyte**](https://www.airbyte.com) &ensp; our **data integration** tool ([try it yourself](https://docs.airbyte.com/quickstart/deploy-airbyte))\n",
    "\n",
    "\n",
    "[Octavia](https://airbyte.com/blog/how-we-chose-our-logo-and-mascot), good friend of Marvin, helps us in moving batch data A ‚Üí B.\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/airbyte-connectors.png\" width=600/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color: #0d4a73;border-radius: 10px;padding: 20px;\">\n",
    "\n",
    "üí° **note**:\n",
    "\n",
    "Make sure your instance is an accessible network space.\n",
    "\n",
    "For example, I'm running airbyte on an [AWS EC2 instance](https://aws.amazon.com/pm/ec2) and port-forwarding the SSH connection to my `localhost:8000`) with the following command: \n",
    "\n",
    "<code>$ ssh demo-airbyte -L 8000:localhost:8000 </code>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <img src=\"imgs/dbt-logo.png\" width=30/>  &emsp; [**dbt (data build tool)**](https://www.getdbt.com/) &ensp; our **transformation** layer\n",
    "\n",
    "dbt takes raw data sources that live in a data warehouse and transforms them into reporting-ready data sources.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/dbt-schematic.png\" width=1000/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <img src=\"imgs/snowflake-logo.webp\" width=33/> &ensp; [**Snowflake**](https://signup.snowflake.com) &ensp; our **data warehouse** ([try it yourself](https://signup.snowflake.com))\n",
    "\n",
    "Our data warehouse is the place data lives:\n",
    "\n",
    "<center>\n",
    "    <img src=\"imgs/snowflake-schematic.png\" width=1000/>\n",
    "</center>\n",
    "\n",
    "... using some process like flow below, we fill our warehouse with raw data and use DBT to run SQL against it, sculpting clean and meaningful data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's go! my basic data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in Python, it ways starts with imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "\n",
    "import json, requests, subprocess, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be making some API calls and dealing in JSON, so let's define some..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a `task` that can trigger a sync of our airbyte connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on EC2, port forwarded to local via EIP\n",
    "@task(name='trigger airbtye sync')\n",
    "def trigger_airbyte_sync(connectionId: str) -> bool:\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url='http://localhost:8000/api/v1/connections/sync',\n",
    "            headers=headers,\n",
    "            data=json.dumps({\"connectionId\": connectionId})\n",
    "        )\n",
    "        assert response.ok\n",
    "\n",
    "        job_id = response.json()[\"job\"][\"id\"]\n",
    "        job_status = ''\n",
    "        \n",
    "        while job_status != 'succeeded':\n",
    "            job_response = requests.post(\n",
    "                url='http://localhost:8000/api/v1/jobs/get/',\n",
    "                headers=headers,\n",
    "                data=json.dumps({\"id\": job_id})\n",
    "            )\n",
    "            \n",
    "            job_status = job_response.json()[\"job\"][\"status\"]\n",
    "            print(job_status)\n",
    "            time.sleep(5)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except AssertionError:\n",
    "        print('Bad Response')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now a `task` that will run our dbt transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbt running locally, connected to my snowflake instance\n",
    "@task(name='trigger dbt job')\n",
    "def trigger_dbt_job(dbt_command: str) -> None:\n",
    "    \n",
    "    subprocess.run(dbt_command.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #0d4a73;border-radius: 10px;padding: 20px;\">\n",
    "\n",
    "üí° **note**:\n",
    "\n",
    "`subprocess` is a standard library package to run shell commands using Python, `subprocess.run` takes `List[str]` like `[\"cmd\", \"-arg\"]`\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name='My Basic Data Pipeline')\n",
    "def my_flow(airbyte_connection_id: str, dbt_command: str) -> None:\n",
    "    \n",
    "    sync = trigger_airbyte_sync(airbyte_connection_id)\n",
    "\n",
    "    if sync.result():\n",
    "        trigger_dbt_job(dbt_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbyte_connection_id = 'e1b2078f-882a-4f50-9942-cfe34b2d825b'\n",
    "dbt_command = 'dbt run --project-dir my_dbt_project'\n",
    "\n",
    "my_flow(\n",
    "    airbyte_connection_id, \n",
    "    dbt_command\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go look at what happened to our data in snowflake!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
