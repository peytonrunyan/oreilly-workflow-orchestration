{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Dask\n",
    "\n",
    "[Dask](https://dask.org/) is an open-source distributed computing framework built to extend the PyData stack. [This issue](https://github.com/dask/dask/issues/4471) on the Dask repo has some architecture diagrams that illustrate Dask's architecture.\n",
    "\n",
    "Most commonly, people are introduced to Dask as a means to scale Pandas. Pandas can be inefficient because a lot of Pandas operations generate [intermediate copies](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets) of data, utilizing more memory than necessary. To effectively handle data with pandas, users preferably need to have [5x to 10x times](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) as much RAM as the size of the dataset. \n",
    "\n",
    "Dask scales Pandas by creating a higher level of Pandas DataFrames called the Dask DataFrame. The image below shows this architecture. Each Pandas DataFrame here is referred to as a partition, which is a logical (and physical) grouping of data.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/306380/129031375-83547ea2-b3fd-4623-ad9a-e57ddc23a9e6.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask DataFrame allows us to utilize an interface similar to Pandas to interact with data that lives across a cluster. The following diagram shows the architecture of a Dask cluster.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/306380/129031416-fe117b62-83f6-47ce-9227-ba8a50db3bf8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Client can give on your local computer, and can submit tasks to the Dask cluster. The Schedeuler is the entrypoint that receives this task and decides which worker to send it to. When using the Dask DataFrame API, Dask handles the lower level managing of sending partitions to workers or rearranging the data across the cluster. This is called a shuffle in distributed compute terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask collections\n",
    "\n",
    "But while we have discussed this in the context of DataFrames, the DataFrame concept is just one of the available collections. The diagram below shows the other available collections.\n",
    "\n",
    "![img](https://user-images.githubusercontent.com/306380/129031388-6fcd0cd1-9643-4f3d-b4b1-c6e343bbcf08.jpg)\n",
    "\n",
    "The Dask Bag is like a distributed dictionary or JSON. The Dask Array builds on top of `xarray`. But the collection that is most relevant to workflow orchestration are Futures.\n",
    "\n",
    "## Dask Futures\n",
    "\n",
    "Dask Delayed and Dask Futures allow for the submission of arbitrary code to the DAsk scheduler. The Dask scheduler then directs the execution to a worker. The difference is that Dask Delayed is evaluated lazily, allowing the computation graph to compile before execution. Knowing the execution graph ahead of time allows Dask to optimize it by analyzing the data depdenencies and ensuring that workers have all the dependencies they need to execute tasks. Dask Futures, on the other hand are executed immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stuff to add: compute-bound versus memory bound problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dask DAG versus the Prefect DAG\n",
    "\n",
    "So what is the relationship of the Dask DAG and the Prefect DAG? The Prefect DAG uses Dask Futures because it needs the eager execution. For example Prefect has a flow with the following structure:\n",
    "\n",
    "```python\n",
    "@flow\n",
    "def myflow():\n",
    "    a = task_one()\n",
    "    b = task_two(a)\n",
    "    c = task_three(b)\n",
    "```\n",
    "\n",
    "Prefect, independent of Dask, sorts the DAG and determines the order to run the tasks. The thing is that a failure in `task_one` will cause Prefect to not trigger `task_two` and `task_three`. In that sense, the Prefect DAG is what handles the monitoring of state throughout the data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does Prefect use Dask?\n",
    "\n",
    "Spark and Dask as Python frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping in Prefect 1.0\n",
    "\n",
    "Depth first execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting-up a Dask cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason why Dask is widely adopted is because of the ease of spinning up your own ephemeral cluster for the duration of an application (or flow run). [This documentation page](https://docs.dask.org/en/stable/deploying.html) contains the various ways that a Dask cluster can be deployed. The choice of cluster largely depeneds on your infrastructure. The most commons ones are:\n",
    "\n",
    "* Dask on Kubernetes\n",
    "* Dask on AWS/ECS or Fargate\n",
    "* Managed services like [Coiled](https://coiled.io/) or [Saturn Cloud](https://saturncloud.io/)\n",
    "\n",
    "The important thing to note in distributed systems is that the package versions on the workers need to be the same as the scheduler and client. Otherwise, it's very easy to run into inconsistent execution, or programs will raise an error. Most Dask cluster initialization method will take in a Docker base image that can be used to spin-up the workers. This guarantees execution.\n",
    "\n",
    "For example, this is what spinning up a Dask cluster looks like with a `KubeCluster`\n",
    "\n",
    "```python\n",
    "from dask_kubernetes import KubeCluster, make_pod_spec\n",
    "\n",
    "pod_spec = make_pod_spec(image='prefecthq/prefect:latest')\n",
    "cluster = KubeCluster(pod_spec)\n",
    "cluster.scale(10)\n",
    "```\n",
    "\n",
    "**This is why it's important to know how to build your own image to include the dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Dask?\n",
    "Dask is a flexible library for parallel computing Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What makes Dask good for distrubted compute? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth-first execution/mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "from prefect.task_runners import DaskTaskRunner\n",
    "from typing import List\n",
    "import httpx\n",
    "\n",
    "@task(retries=3)\n",
    "def get_stars(repo: str):\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    count = httpx.get(url).json()[\"stargazers_count\"]\n",
    "    print(f\"{repo} has {count} stars!\")\n",
    "\n",
    "@flow(name=\"Github Stars\", task_runner=DaskTaskRunner())\n",
    "def github_stars(repos: List[str]):\n",
    "    for repo in repos:\n",
    "        get_stars(repo)\n",
    "\n",
    "# run the flow!\n",
    "if __name__ == \"__main__\":\n",
    "    github_stars([\"PrefectHQ/Prefect\", \"PrefectHQ/miter-design\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
