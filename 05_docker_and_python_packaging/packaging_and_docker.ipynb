{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docker and Python Packaging\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/504/1*iBGlEPUruUqqT5NreeEF8g.png\" width=200>\n",
    "\n",
    "## Motivation\n",
    "\n",
    "In the previous section, we saw how to spin up a cluster using Dask. Using a Docker image spins up the workers with a consistent environment so that distributed flows run reliably. But even beyond the use case with Dask, Docker is an integral part of data workflows. Data professionals often hear phrases like \"it worked on my machine\" or \"nothing changed, but by workflow stopped running.\"\n",
    "\n",
    "Docker allows us to build images and run them in other execution environments. An image contains all of the dependencies needed for our application, while a container is an instance of the image. By building a fixed image, we can pin the dependencies so that they stay fixed from run to run. \n",
    "\n",
    "## When to use Docker?\n",
    "\n",
    "Data scientists or data engineers might already be familiar with virtual environments. Virtual environment managers such as `poetry`, `pipenv`, and `conda` allow us to fix package versions. What are the use cases then that call for Docker?\n",
    "\n",
    "1. You can have multiple containers with different Python versions\n",
    "2. It allows us to pin the non-Python dependencies such as Java for Spark\n",
    "3. It is the unit for spinning up clusters/jobs (Kubernetes or Dask)\n",
    "4. Host legacy applications with older technology (some Prefect users run containers with specific scientific computing libraries)\n",
    "5. Encourages reproducible work\n",
    "6. Eases the transition from development to deployment (some services like AWS ECS and Google Vertex need containers to run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Architecture\n",
    "\n",
    "The Aqua Security [documentation](https://www.aquasec.com/cloud-native-academy/docker-container/docker-architecture/) has a very good diagram about the Docker architecture. There are three main parts:\n",
    "\n",
    "* Client - this can be the Docker client or Python client\n",
    "* Daemon - the daemon is what orchestrates containers and images\n",
    "* Registry - used to store images for downloading in other places.\n",
    "\n",
    "![img](docker_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Project\n",
    "\n",
    "Alongside this notebook, there is a folder named `docker_with_custom_module`. It represents a Prefect Flow that uses custom modules defined by the user. Packaging custom code as a Python modules allows us to run with from any directory within the container, as well as reuse it in other projects more easily. Using this folder, we will build a custom Docker image to support our Flow. Below is the directory structure.\n",
    "\n",
    "```\n",
    "docker_with_custom_module/\n",
    "├── components/\n",
    "│   ├── __init__.py\n",
    "│   ├── componentA.py\n",
    "│   ├── componentB.py\n",
    "├── workflow/\n",
    "│   ├── custom_flow.py\n",
    "├── requirements.txt\n",
    "├── Dockerfile\n",
    "└── setup.py\n",
    "```\n",
    "\n",
    "and a brief description of each of the components:\n",
    "\n",
    "* components - contains the custom code that will be used in multiple flows\n",
    "* workflow - contains the Prefect flow\n",
    "* requirements.txt - dependencies of the project\n",
    "* Dockerfile - the instructions to package this folder into a Docker image\n",
    "* setup.py - `pip` looks at this file for instructions how to install the module\n",
    "\n",
    "### Quick look at the Python code\n",
    "\n",
    "The components are very simple Python classes. We just want to create something we can import for the main flow. Below is `componentA.py`. `componentB.py` is very similar.\n",
    "\n",
    "```python\n",
    "class ComponentA:\n",
    "    def __init__(self, n=2) -> None:\n",
    "        self.n = n\n",
    "```\n",
    "\n",
    "From there, we can take a look at the `custom_flow.py` in the `workflow` folder. This just imports the components and uses them inside a task.\n",
    "\n",
    "```python\n",
    "from prefect import flow, task\n",
    "\n",
    "from components.componentA import ComponentA \n",
    "from components.componentB import ComponentB\n",
    "\n",
    "@task\n",
    "def custom_task():\n",
    "    x = ComponentA(2)\n",
    "    y = ComponentB(2)\n",
    "    _sum = x.n + y.n\n",
    "    print(f\"Test {_sum}!\")  # Should return 4\n",
    "    return _sum\n",
    "\n",
    "@flow\n",
    "def custom_flow():\n",
    "    custom_task()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup.py\n",
    "\n",
    "In order to package the `components` as a module, we need to add the `__init__.py` file inside the folder. The package name and version are used by pip to keep track of the package, but they don’t affect how the package is used in Python code. The `find_packages()` function call goes through the subdirectories with an `__init__.py` and includes them in `mypackage`. Notice this file takes care of installing the requirements. The `setup()` function is the one `pip` looks for in order to install the library.\n",
    "\n",
    "```python\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "with open('requirements.txt') as f:\n",
    "    requirements = f.read().splitlines()\n",
    "\n",
    "setup(\n",
    "    name=\"mypackage\",\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=requirements\n",
    ")\n",
    "```\n",
    "\n",
    "### Installing the custom module\n",
    "\n",
    "With this file written, we can now install the library by doing,\n",
    "\n",
    "```\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "and this lets us import `components` from other directories because the Python path can resolve it.\n",
    "\n",
    "## Building the Docker image\n",
    "\n",
    "Now that we have the custom module installed, we want to create the Docker image so that we can run it in other execution environments. \n",
    "\n",
    "```Dockerfile\n",
    "FROM prefecthq/prefect:latest\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "ADD . .\n",
    "\n",
    "RUN pip install .\n",
    "```\n",
    "\n",
    "and that is it we need. Below is the explanation for each line:\n",
    "\n",
    "1. FROM — this is the base image that we’ll be using for our image. Projects such as Spark or Dask. These are especially useful when you are using tools that are not confined to Python. For example, Spark needs Java in the container also, and using the base Spark image takes are of that.\n",
    "2. WORKDIR — set the working directory for the container. It will be created if it doesn’t exist\n",
    "3. ADD — here we add all of our files from the current directory to the container WORKDIR\n",
    "4. RUN — this is where we install our library (-e is not really needed as that’s for development). This will also install all requirements because of the way we structured our setup.py file earlier.\n",
    "\n",
    "In order to build the image, we can run a command like the following:\n",
    "\n",
    "```\n",
    "docker build . -t test:latest\n",
    "```\n",
    "\n",
    "where `test` is the image name and `latest` is the image tag.\n",
    "\n",
    "### Using the image\n",
    "\n",
    "In order to check everything is good, we can run the image interactively,\n",
    "\n",
    "```\n",
    "docker run --name containername -i -t test:latest sh\n",
    "```\n",
    "\n",
    "and from there we should be in the app directory and we can run our flow with:\n",
    "\n",
    "```\n",
    "python workflow/flow.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to a Registry\n",
    "\n",
    "Now that the image has been created, you can push it to your registry (Dockerhub, AWS ECR, etc.) using the `docker push` command. These registries will have different ways to do it but we'll cover how to do it with DockerHub, which is the de facto registry.\n",
    "\n",
    "### Auth\n",
    "We need to auth our CLI session with our image repository otherwisesuperconvenientfreestorage\n",
    "\n",
    "```console\n",
    "docker login\n",
    "```\n",
    "\n",
    "### Build and Tag\n",
    "```console\n",
    "docker build . --tag zzstoatzz/prefect-imgs:dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the image for a Flow\n",
    "\n",
    "In order to use the image for a flow, we can create a deployment using the `DockerFlowRunner()` with the image that we just uploaded\n",
    "\n",
    "```python\n",
    "from prefect.deployments import DeploymentSpec\n",
    "from prefect.flow_runners import DockerFlowRunner\n",
    "\n",
    "DeploymentSpec(\n",
    "    name=\"docker-example\",\n",
    "    flow=custom_flow,\n",
    "    flow_runner=DockerFlowRunner(\"repo/image\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we'll look at advanced patterns for workflow orchestration before doing an end-to-end example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
